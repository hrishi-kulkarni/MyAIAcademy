{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Cleaning Text\n",
    "Most basic text cleaning operations should only use Python’s core string operations,\n",
    "in particular strip, replace, and split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = [\" Interrobang. By HK Henriette \",\n",
    "                \"Parking And Going. By Karl Gautier\",\n",
    "                \" Today Is The night. By Jarek Prakash \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interrobang. By HK Henriette',\n",
       " 'Parking And Going. By Karl Gautier',\n",
       " 'Today Is The night. By Jarek Prakash']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_without_whitespace = [string.strip() for string in text_data]\n",
    "string_without_whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interrobang By HK Henriette',\n",
       " 'Parking And Going By Karl Gautier',\n",
       " 'Today Is The night By Jarek Prakash']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_without_period = [string.replace(\".\", \"\") for string in string_without_whitespace]\n",
    "string_without_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INTERROBANG BY HK HENRIETTE',\n",
       " 'PARKING AND GOING BY KARL GAUTIER',\n",
       " 'TODAY IS THE NIGHT BY JAREK PRAKASH']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capital_text = [string.upper() for string in string_without_period]\n",
    "capital_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Parsing and cleaning HTML\n",
    "## Problem:\n",
    "You have text data with HTML elements and want to extract just the text.\n",
    "\n",
    "## Solution:\n",
    "Use Beautiful Soup’s extensive set of options to parse and extract from HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMasego Azra'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sample HTML content\n",
    "html = \"\"\"\n",
    "<div class='full_name'><span style='font-weight:bold'>\n",
    "Masego</span> Azra</div>\"\n",
    "\"\"\"\n",
    "\n",
    "# Parse html\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "#Find div with class name 'full_name'\n",
    "soup.find('div', {'class':'full_name'}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the strange name, Beautiful Soup is a powerful Python library designed for\n",
    "scraping HTML. Typically Beautiful Soup is used scrape live websites, but we can just\n",
    "as easily use it to extract text data embedded in HTML. The full range of Beautiful\n",
    "Soup operations is beyond the scope of this blog, but even the few methods used in\n",
    "our solution show how easily we can parse HTML code to extract the data we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Removing punctuation\n",
    "## Problem:\n",
    "You have a feature of text data and want to remove punctuation.\n",
    "\n",
    "## Solution:\n",
    "Define a function that uses translate with a dictionary of punctuation characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi I Love This Song', '10000 Agree LoveIT', 'Right']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "text_data = ['Hi!!!! I. Love. This. Song....',\n",
    "            '10000% Agree!!!! #LoveIT',\n",
    "            'Right?!?!']\n",
    "res_list = []\n",
    "for string_tmp in text_data:\n",
    "    res = ''\n",
    "    for c in string_tmp:\n",
    "        if c not in string.punctuation:\n",
    "            res = res+c\n",
    "    res_list.append(res)\n",
    "res_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to be conscious of the fact that punctuation contains information (e.g.,\n",
    "“Right?” versus “Right!”). Removing punctuation is often a necessary evil to create\n",
    "features; however, if the punctuation is important we should make sure to take that\n",
    "into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Tokenizing text\n",
    "## Problem:\n",
    "You have text and want to break it up into individual words.\n",
    "\n",
    "## Solution:\n",
    "NLP Toolkit of python (NLTK) has some really powerful methods including text tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'science',\n",
       " 'of',\n",
       " 'today',\n",
       " 'is',\n",
       " 'the',\n",
       " 'technology',\n",
       " 'of',\n",
       " 'tomorrow',\n",
       " '.',\n",
       " 'Thus',\n",
       " 'the',\n",
       " 'end']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "string = \"The science of today is the technology of tomorrow. Thus the end\"\n",
    "\n",
    "word_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization, especially word tokenization, is a common task after cleaning text data\n",
    "because it is the first step in the process of turning the text into data we will use to\n",
    "construct useful features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Removing stop words\n",
    "## Problem:\n",
    "Given tokenized words, remove common words.\n",
    "\n",
    "## Solution:\n",
    "Use NLTK's stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['going', 'market', 'today', 'enjoyed', 'ride']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I am going to market today I had enjoyed your ride\"\n",
    "text = text.lower()\n",
    "list_of_words = word_tokenize(text)\n",
    "\n",
    "tmp = stopwords.words('english')\n",
    "text_without_stopwords = [word for word in list_of_words if word not in tmp]\n",
    "\n",
    "text_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While “stop words” can refer to any set of words we want to remove before processing,\n",
    "frequently the term refers to extremely common words that themselves contain\n",
    "little information value. NLTK has a list of common stop words that we can use to\n",
    "find and remove stop words in our tokenized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[:10]    #Here tmp is list of stopwords as assigned in above cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note that NLTK’s stopwords assumes the tokenized words are all lowercased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Stemming words\n",
    "You have tokenized words and want to convert them into their root forms.\n",
    "\n",
    "## Problem:\n",
    "You have tokenized words and want to convert them into their root forms.\n",
    "\n",
    "## Solution:\n",
    "Use NLTK’s PorterStemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'market', 'today', 'enjoy', 'ride']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Here we already have tokenized words from above output, i.e. text_without_stopwords\n",
    "\n",
    "#Create porter\n",
    "porter = PorterStemmer()\n",
    "\n",
    "root_words = [porter.stem(word) for word in text_without_stopwords]\n",
    "root_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming reduces word to its stem - without altering the original meaning of word. For example, both “tradition” and\n",
    "“traditional” have “tradit” as their stem, indicating that while they are different words\n",
    "they represent the same general concept. This process removes suffixes, prefixes etc. to get original root words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Tagging Parts of Speech\n",
    "## Problem:\n",
    "You have text data and want to tag each word or character with its part of speech.\n",
    "\n",
    "## Solution:\n",
    "Use NLTK’s pre-trained parts-of-speech tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = \"Chris loved outdoor running\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use pre-trained part of speech tagger\n",
    "text_tagged = pos_tag(word_tokenize(text_data))\n",
    "text_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a list of tuples with the word and the tag of the part of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here is the list of sample acronyms and their meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "NNP: Proper noun, singular\n",
    "NN: Noun, singular or mass\n",
    "RB: Adverb\n",
    "VBD: Verb, past tense\n",
    "VBG: Verb, gerund or present participle\n",
    "JJ: Adjective\n",
    "PRP: Personal pronoun\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've tags for each word, we can use it to find certain parts of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chris']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example to get all nouns\n",
    "nouns = [word for word, tag in text_tagged if tag in {'NN','NNS','NNP','NNPS'}]\n",
    "nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realistic case study of use of Parts-of-speech:\n",
    "Consider we have data where every observation has tweet. We want to convert those sentences into features for individual parts of speech. (e.g., a feature with 1 if a proper noun is present, and 0 otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\"I am eating a burrito for breakfast\",\n",
    "        \"Political science is an amazing field\",\n",
    "        \"San Francisco is an awesome city\"]\n",
    "\n",
    "# Create list\n",
    "tagged_tweets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['PRP', 'VBP', 'VBG', 'DT', 'NN', 'IN', 'NN'],\n",
       " ['JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NN'],\n",
       " ['NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tag each word and each tweet\n",
    "for tweet in tweets:\n",
    "    tweet_tag = nltk.pos_tag(word_tokenize(tweet))\n",
    "    tagged_tweets.append([tag for word, tag in tweet_tag])\n",
    "    \n",
    "tagged_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using one hot encoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Use one-hot encoding to convert the tags into features\n",
    "one_hot_multi = MultiLabelBinarizer()\n",
    "one_hot_multi.fit_transform(tagged_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using classes_ we can see that each feature is part-of-speech-tag\n",
    "one_hot_multi.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Encoding Text as a Bag of Words\n",
    "## Problem:\n",
    "You have text data and want to create a set of features indicating the number of times\n",
    "an observation’s text contains a particular word.\n",
    "\n",
    "## Solution:\n",
    "Use scikit-learn’s CountVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create text\n",
    "text_data = np.array(['I love India. India!',\n",
    "                        'Pune is the best',\n",
    "                        'xyz beats both'])\n",
    "\n",
    "# Create the bag of words feature matrix\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "\n",
    "# Show feature matrix\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "This output is a sparse array, which is often necessary when we have a large amount\n",
    "of text. However, in our toy example we can use toarray to view a matrix of word\n",
    "counts for each observation:\n",
    "'''\n",
    "\n",
    "bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### One of the most common methods of transforming text into features is by using a bag-of-words model. Bag-of-words models output a feature for every unique word in text data, with each feature containing a count of occurrences in observations.\n",
    "###### The text data in our solution was purposely small. In the real world, a single observation of text data could be the contents of an entire book! Since our bag-of-words model creates a feature for every unique word in the data, the resulting matrix can contain thousands of features. This means that the size of the matrix can sometimes become very large in memory. However, luckily we can exploit a common characteristic of bag-of-words feature matrices to reduce the amount of data we need to store.\n",
    "###### Most words likely do not occur in most observations, and therefore bag-of-words feature matrices will contain mostly 0s as values. We call these types of matrices “sparse.” Instead of storing all values of the matrix, we can only store nonzero values and then assume all other values are 0. This will save us memory when we have large feature matrices. One of the nice features of CountVectorizer is that the output is a sparse matrix by default.\n",
    "###### CountVectorizer comes with a number of useful parameters to make creating bag-of-words feature matrices easy. First, while by default every feature is a word, that does not have to be the case. Instead we can set every feature to be the combination of two words (called a 2-gram) or even three words (3-gram). ngram_range sets the minimum and maximum size of our n-grams. For example, (2,3) will return all 2- grams and 3-grams. Second, we can easily remove low-information filler words using stop_words either with a built-in list or a custom list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 2, 1, 1, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create feature matrix with arguments\n",
    "count_2gram = CountVectorizer(ngram_range=(1,2),\n",
    "    stop_words=\"english\")\n",
    "\n",
    "bag = count_2gram.fit_transform(text_data)\n",
    "# View feature matrix\n",
    "bag.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 4,\n",
       " 'india': 2,\n",
       " 'love india': 5,\n",
       " 'india india': 3,\n",
       " 'pune': 6,\n",
       " 'best': 1,\n",
       " 'pune best': 7,\n",
       " 'xyz': 8,\n",
       " 'beats': 0,\n",
       " 'xyz beats': 9}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_2gram.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AND THE MOST IMPORTANT CONCEPT OF TEXT HANDLING ->\n",
    "# * Weighting word importance\n",
    "## Problem:\n",
    "You want a bag of words, but with words weighted by their importance to an observation.\n",
    "\n",
    "## Solution:\n",
    "Compare the frequency of the word in a document (i.e., the dataset of tweets, movie reviews, speech\n",
    "transcripts, etc.) with the frequency of the word in all other documents using term\n",
    "frequency-inverse document frequency (tf-idf). scikit-learn makes this easy with\n",
    "TfidfVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requirements:\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "text_data = np.array(['I love India. India!',\n",
    "                        'Japan is the best',\n",
    "                        'NY beats both'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.89442719, 0.        ,\n",
       "        0.        , 0.4472136 , 0.        , 0.        ],\n",
       "       [0.        , 0.5       , 0.        , 0.        , 0.5       ,\n",
       "        0.5       , 0.        , 0.        , 0.5       ],\n",
       "       [0.57735027, 0.        , 0.57735027, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.57735027, 0.        ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the tf-idf feature matrix\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "\n",
    "feature_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more a word appears in a document, the more likely it is important to that document.\n",
    "For example, if the word economy appears frequently, it is evidence that the\n",
    "document might be about economics. We call this term frequency (tf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, if a word appears in many documents, it is likely less important to any\n",
    "individual document. For example, if every document in some text data contains the\n",
    "word 'after', then it is probably an unimportant word. We call this document frequency\n",
    "(df)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By combining these two statistics, we can assign a score to every word representing\n",
    "how important that word is in a document. Specifically, we multiply tf to the inverse\n",
    "of document frequency (idf):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf_idf(t, d) = tf(t,d) × idf(t)\n",
    "where t is a word and d is a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The higher the resulting value, the more important the word is to a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it folks... These are basics about handling text data in data mining process. You can get more information from post at AnalyticsVidhya:\n",
    "https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/\n",
    "\n",
    "# See you next week!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
